{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rand 2011 Cooperation Study\n",
    "\n",
    "This notebook outlines how to recreate the analysis of the Rand et al. 2011 study **\"Dynamic social networks promote cooperation in experiments with humans\"** [Link to Paper](http://humannaturelab.net/wp-content/uploads/2014/10/126-Dynamic-Social-Networks-Promote-Cooperation-in-Experiments-with-Humans.pdf \"Full PDF\")\n",
    "\n",
    "This outlines the steps to re-create the analysis using the publicly available data published in the paper.  This requires either a local or remote copy of Bedrock with the following Opals installed:\n",
    "\n",
    "* [Spreadsheet](https://github.com/Bedrock-py/opal-dataloader-ingest-spreadsheet)\n",
    "* [logit2]()\n",
    "* [select-from-dataframe](https://github.com/Bedrock-py/opal-analytics-select-from-dataframe)\n",
    "\n",
    "This notebook also requires that bedrock-core be installed locally into the python kernel running this notebook.  This can be installed via command line using:\n",
    "\n",
    "`pip install git+https://github.com/Bedrock-py/bedrock-core.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check that Bedrock is installed locally.  If the following cell does not run without error, check the install procedure above and try again.  Also, ensure that the kernel selected is the same as the kernel where bedrock-core is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bedrock.client.client import BedrockAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Connection to Bedrock Server\n",
    "\n",
    "This code assumes a local bedrock is hosted at localhost on port 81.  Change the `SERVER` variable to match your server's URL and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "SERVER = \"http://localhost:81/\"\n",
    "api = BedrockAPI(SERVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Spreadsheet Opal\n",
    "\n",
    "The following code block checks the Bedrock server for the Spreadsheet Opal.  This Opal is used to load .csv, .xls, and other such files into a Bedrock matrix format.  The code below calls the Bedrock `/dataloaders/ingest` endpoint to check if the `opals.spreadsheet.Spreadsheet.Spreadsheet` opal is installed.\n",
    "\n",
    "If the code below shows the Opal is not installed, there are two options:\n",
    "1. If you are running a local Bedrock or are the administrator of the Bedrock server, install the Spreadsheet Opal with pip on the server [Spreadsheet](https://github.com/Bedrock-py/opal-dataloader-ingest-spreadsheet)\n",
    "2. If you are not administrator of the Bedrock server, e-mail the Bedrock administrator requesting the Opal be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = api.get(\"dataloader\",\"ingest\")\n",
    "dataloader_opals = resp.json()\n",
    "spreadsheet_opals = filter(lambda opal: opal['ingest_id'] == 'opals.spreadsheet.Spreadsheet.Spreadsheet', dataloader_opals)\n",
    "if next(spreadsheet_opals,'None') != 'None':\n",
    "    print(\"Spreadsheet Opal Installed!\")\n",
    "else:\n",
    "    print(\"Spreadsheet Opal Not Installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for logit2 Opal\n",
    "\n",
    "The following code block checks the Bedrock server for the logit2 Opal. \n",
    "\n",
    "If the code below shows the Opal is not installed, there are two options:\n",
    "1. If you are running a local Bedrock or are the administrator of the Bedrock server, install the logit2 Opal with pip on the server [logit2]()\n",
    "2. If you are not administrator of the Bedrock server, e-mail the Bedrock administrator requesting the Opal be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = api.get(\"analytics\",\"analytics\")\n",
    "analytic_opals = resp.json()\n",
    "logit2_opals = filter(lambda opal: opal['analytic_id'] == 'opals.logit2.Logit2.Logit2', analytic_opals)\n",
    "if next(logit2_opals,'None') != 'None':\n",
    "    print(\"Logit2 Opal Installed!\")\n",
    "else:\n",
    "    print(\"Logit2 Opal Not Installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for select-from-dataframe Opal\n",
    "\n",
    "The following code block checks the Bedrock server for the select-from-dataframe Opal. This allows you to filter by row and reduce the columns in a dataframe loaded by the server. \n",
    "\n",
    "If the code below shows the Opal is not installed, there are two options:\n",
    "1. If you are running a local Bedrock or are the administrator of the Bedrock server, install the select-from-datafram Opal with pip on the server [select-from-dataframe](https://github.com/Bedrock-py/opal-analytics-select-from-dataframe)\n",
    "2. If you are not administrator of the Bedrock server, e-mail the Bedrock administrator requesting the Opal be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = api.get(\"analytics\",\"analytics\")\n",
    "analytic_opals = resp.json()\n",
    "select_from_dataframe_opals = filter(lambda opal: opal['analytic_id'].startswith('opals.select-from-dataframe'), analytic_opals)\n",
    "if next(select_from_dataframe_opals,'None') != 'None':\n",
    "    print(\"Select-from-dataframe Opal Installed!\")\n",
    "else:\n",
    "    print(\"Select-from-dataframe Opal Not Installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Data to Bedrock and Create Matrix\n",
    "\n",
    "Now that everything is installed, begin the workflow by uploading the csv data and creating a matrix.  To understand this fully, it is useful to understand how a data loading workflow occurs in Bedrock.\n",
    "\n",
    "1. Create a datasource that points to the original source file\n",
    "2. Generate a matrix from the data source (filters can be applied during this step to pre-filter the data source on load\n",
    "3. Analytics work on the generated matrix\n",
    "\n",
    "** Note: Each time a matrix is generated from a data source it will create a new copy with a new UUID to represent that matrix **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for csv file locally\n",
    "\n",
    "The following code opens the file and prints out the first part.  The file must be a csv file with a header that has labels for each column.  The file is comma delimited csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = 'Rand2011PNAS_cooperation_data.csv'\n",
    "import csv\n",
    "with open(filepath,'r') as f:\n",
    "    print(f.readlines(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Upload the source file to the Bedrock Server\n",
    "\n",
    "This code block uses the Spreadsheet ingest module to upload the source file to Bedrock.  ** Note: This simply copies the file to the server, but does not create a Bedrock Matrix format **\n",
    "\n",
    "If the following fails to upload. Check that the csv file is in the correct comma delimited format with headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ingest_id = 'opals.spreadsheet.Spreadsheet.Spreadsheet'\n",
    "resp = api.put_source('Rand2011', ingest_id, 'default', {'file': open(filepath, \"rb\")})\n",
    "\n",
    "if resp.status_code == 201:\n",
    "    source_id = resp.json()['src_id']\n",
    "    print('Source {0} successfully uploaded'.format(filepath))\n",
    "else:\n",
    "    print('Failed to upload {0}'.format(filepath)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check available data sources for the CSV file\n",
    "\n",
    "Call the Bedrock sources list to see available data sources.  Note, that the `Rand2011` data source should now be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "available_sources = api.list(\"dataloader\", \"sources\").json()\n",
    "s = next(filter(lambda source: source['src_id'] == source_id, available_sources),'None')\n",
    "if s != 'None':\n",
    "    print(s)\n",
    "else:\n",
    "    print(\"Could not find source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at basic statistics on the source data\n",
    "\n",
    "Here we can see that Bedrock has computed some basic statistics on the source data.\n",
    "\n",
    "#### For numeric data\n",
    "\n",
    "The quartiles, max, mean, min, and standard deviation are provided\n",
    "\n",
    "#### For non-numeric data\n",
    "\n",
    "The label values and counts for each label are provided.\n",
    "\n",
    "#### For both types\n",
    "\n",
    "The proposed tags and data type that Bedrock is suggesting are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "endpoint = api.endpoint(\"dataloader\", \"sources/%s/explore/\" % source_id)\n",
    "sources_list = requests.get(endpoint).json()\n",
    "rand2011_source = sources_list['Rand2011PNAS_cooperation_data']\n",
    "pandas.DataFrame.from_dict(rand2011_source['fields'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Bedrock Matrix from the CSV Source\n",
    "\n",
    "In order to use the data, the data source must be converted to a Bedrock matrix.  The following code steps through that process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Provide the source data name\n",
    "source_name = 'Rand2011PNAS_cooperation_data'\n",
    "\n",
    "# Give the matrix an id and name\n",
    "matrix_id = 'rand_mtx'\n",
    "matrix_name = 'rand_mtx'\n",
    "\n",
    "# Create a list of names for the features.  Note this is based on the original ordering of the data when uploaded\n",
    "feature_list = 'sessionnum,condition,playerid,decision0d1c,previous_decision,round_num,num_neighbors,group_size,fluid_dummy,_Icondition_2,_Icondition_3,_Icondition_4'.split(',')\n",
    "\n",
    "# All of the features provided were numeric.  Note that the one categorical field already had dummy variables created\n",
    "column_types = ['Numeric' for x in feature_list]\n",
    "\n",
    "# No filtering on the data before creating the matrix. (Filters include things like transforming data before loading)\n",
    "matrix_filters = dict.fromkeys(feature_list,{})\n",
    "\n",
    "# Create the JSON object for the API endpoint that contains all of the above information\n",
    "matbody = {\n",
    "    'matrixFeatures': feature_list,\n",
    "    'matrixFeaturesOriginal': feature_list,\n",
    "    'matrixFilters': matrix_filters,\n",
    "    'matrixName': matrix_name,\n",
    "    'matrixTypes': column_types,\n",
    "    'sourceName': source_name\n",
    "}\n",
    "\n",
    "# Post to the dataloader/sources/source_id endpoint\n",
    "url = api.endpoint(\"dataloader\", \"sources/%s\" % (source_id))\n",
    "resp = requests.post(url, json=matbody)\n",
    "\n",
    "if resp.status_code == 201:\n",
    "    print(\"Matrix successfully created\")\n",
    "    mtx = resp.json()[0]\n",
    "    matrix_id = mtx['id']\n",
    "else:\n",
    "    print(\"Error creating matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Logit2 Analysis\n",
    "\n",
    "Now we will call the Logit2 Analysis on the matrix.  This will run a logit analysis on the features in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply specified analysis to the matrix\n",
    "analytic_id = \"opals.logit2.Logit2.Logit2\"\n",
    "analysis_postdata = {\n",
    "    'inputs': {\n",
    "        'matrix.csv': mtx,\n",
    "        'features.txt': mtx\n",
    "    },\n",
    "    'name': 'rand-logit2',\n",
    "    'parameters': [{\"attrname\":\"step\",\"value\":\"2\"}],\n",
    "    'src': [mtx]\n",
    "}\n",
    "resp = api.post(\"analytics\", \"analytics/%s\" % analytic_id, json=analysis_postdata)\n",
    "result = resp.json()\n",
    "if resp.status_code == 201:\n",
    "    print(\"Logit2 analysis successful\")\n",
    "else:\n",
    "    print(\"Logit2 analysis failed\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the output of the analysis\n",
    "\n",
    "Here the output of the analysis is downloaded and from here can be visualized and exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download result matrix\n",
    "url = api.endpoint(\"analytics\", \"results/download/%s/%s/%s\" % (result['id'],'matrix.csv','logit2_result.csv'))\n",
    "resp = requests.get(url)\n",
    "print(url)\n",
    "\n",
    "# Display result\n",
    "import pandas\n",
    "print(pandas.DataFrame([x.split(',') for x in resp.text.split(\"\\n\")]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
